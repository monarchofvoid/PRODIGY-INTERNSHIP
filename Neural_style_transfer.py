# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bCDAnp3lULSDiJ7Nyo2iGg8Nyb0SyHaJ
"""

!pip install torch torchvision matplotlib pillow

from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import torch

# Function to load image
def load_image(img_path, max_size=400):
    image = Image.open(img_path).convert('RGB')
    size = min(max(image.size), max_size)
    transform = transforms.Compose([
        transforms.Resize((size, size)),
        transforms.ToTensor()
    ])
    image = transform(image).unsqueeze(0)
    return image

# Load content & style images
content = load_image("/content/Content.jpeg")
style = load_image("/content/Style.jpeg")

# Show them
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.imshow(content.squeeze().permute(1,2,0))
plt.title("Content Image")

plt.subplot(1,2,2)
plt.imshow(style.squeeze().permute(1,2,0))
plt.title("Style Image")
plt.show()

import torch.nn as nn
import torch.optim as optim
from torchvision import models

# Use pretrained VGG19
cnn = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.eval()

# Layers for content & style (using layer indices)
content_layers = [22] # Corresponds to conv4_2
style_layers = [3, 8, 15, 22, 31] # Corresponds to conv1_1, conv2_1, conv3_1, conv4_1, conv5_1

# Extract features
def get_features(image, model, layers):
    features = {}
    x = image
    # Iterate through layers of the VGG model by index
    for i, layer in enumerate(model):
        x = layer(x)
        if i in layers:
            features[i] = x
    return features


# Gram Matrix
def gram_matrix(tensor):
    b, c, h, w = tensor.size()
    features = tensor.view(c, h*w)
    G = torch.mm(features, features.t())
    return G.div(c*h*w)


# Target image (start as content copy)
target = content.clone().requires_grad_(True)

# Optimizer
optimizer = optim.Adam([target], lr=0.003)

# Weights
style_weight = 1e6
content_weight = 1

# Training loop
for step in range(1,201):
    # Get features (compute inside the loop)
    content_features = get_features(content, cnn, content_layers)
    style_features = get_features(style, cnn, style_layers)
    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}

    target_features = get_features(target, cnn, content_layers+style_layers)

    # Content loss (using layer index)
    content_loss = torch.mean((target_features[22] - content_features[22])**2)

    # Style loss (using layer indices)
    style_loss = 0
    for layer_index in style_layers:
        target_feature = target_features[layer_index]
        target_gram = gram_matrix(target_feature)
        style_gram = style_grams[layer_index]
        style_loss += torch.mean((target_gram - style_gram)**2)

    total_loss = content_weight*content_loss + style_weight*style_loss

    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    if step % 50 == 0:
        print(f"Step {step}, Loss: {total_loss.item()}")
        img = target.clone().detach().squeeze()
        img = img.permute(1,2,0).clamp(0,1)
        plt.imshow(img)
        plt.title(f"Step {step}")
        plt.show()